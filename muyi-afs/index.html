<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Programming for Data Analysts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#link-to-python-notebook" id="toc-link-to-python-notebook" class="nav-link" data-scroll-target="#link-to-python-notebook">Link to Python Notebook</a></li>
  <li><a href="#data-source-and-suitability" id="toc-data-source-and-suitability" class="nav-link" data-scroll-target="#data-source-and-suitability">Data Source and Suitability</a></li>
  <li><a href="#user-requirements-and-benefits" id="toc-user-requirements-and-benefits" class="nav-link" data-scroll-target="#user-requirements-and-benefits">User Requirements and Benefits</a></li>
  <li><a href="#code-development-challenges-and-benefits" id="toc-code-development-challenges-and-benefits" class="nav-link" data-scroll-target="#code-development-challenges-and-benefits">Code Development Challenges and Benefits</a></li>
  <li><a href="#regulatory-and-ethical-considerations" id="toc-regulatory-and-ethical-considerations" class="nav-link" data-scroll-target="#regulatory-and-ethical-considerations">Regulatory and Ethical Considerations</a></li>
  </ul></li>
  <li><a href="#approach" id="toc-approach" class="nav-link" data-scroll-target="#approach">Approach</a>
  <ul class="collapse">
  <li><a href="#business-understanding" id="toc-business-understanding" class="nav-link" data-scroll-target="#business-understanding">Business Understanding</a></li>
  <li><a href="#data-understanding" id="toc-data-understanding" class="nav-link" data-scroll-target="#data-understanding">Data Understanding</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data preparation</a></li>
  <li><a href="#code-libraries-used-and-rationale" id="toc-code-libraries-used-and-rationale" class="nav-link" data-scroll-target="#code-libraries-used-and-rationale">Code Libraries Used and Rationale</a></li>
  <li><a href="#choice-of-language-python-and-platform-google-colab" id="toc-choice-of-language-python-and-platform-google-colab" class="nav-link" data-scroll-target="#choice-of-language-python-and-platform-google-colab">Choice of Language (Python) and Platform (Google Colab)</a></li>
  <li><a href="#code-design-and-maintainability" id="toc-code-design-and-maintainability" class="nav-link" data-scroll-target="#code-design-and-maintainability">Code Design and Maintainability</a></li>
  </ul></li>
  <li><a href="#recommendations" id="toc-recommendations" class="nav-link" data-scroll-target="#recommendations">Recommendations</a></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">Reference</a></li>
  <li><a href="#appendices" id="toc-appendices" class="nav-link" data-scroll-target="#appendices">Appendices</a></li>
  <li><a href="#appendix-1---notebook-contents" id="toc-appendix-1---notebook-contents" class="nav-link" data-scroll-target="#appendix-1---notebook-contents">Appendix 1 - Notebook contents</a></li>
  <li><a href="#appendix-3---design-pseudocode" id="toc-appendix-3---design-pseudocode" class="nav-link" data-scroll-target="#appendix-3---design-pseudocode">Appendix 3 - Design Pseudocode</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Programming for Data Analysts</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This report describes the development and impact of a data-driven loan application processing system at Apex Financial Services (AFS). Due to a successful digital marketing effort, the number of loan applications has been steadily rising, which has put pressure on the present manual approval procedure. By using data analytics to partially automate loan processing, this initiative seeks to address these issues and free up the loan team’s valuable time for individualised client engagement.</p>
<section id="link-to-python-notebook" class="level3">
<h3 class="anchored" data-anchor-id="link-to-python-notebook">Link to Python Notebook</h3>
<p><a href="https://colab.research.google.com/drive/1w2mjR42x3O1qv3BCjpvTvp35d2QORhUx?usp=sharing">click here</a></p>
</section>
<section id="data-source-and-suitability" class="level3">
<h3 class="anchored" data-anchor-id="data-source-and-suitability">Data Source and Suitability</h3>
<p>The internal database administrator (DBA) supplied the data for this study from two files: “<strong>Loans_Database_Table.pdf</strong>,” a PDF file, and “<strong>Apex Loans Data.xlsx</strong>,” an Excel file. The PDF provides historical information from the prior year’s company loan applications, including a loan approval status field (Yes/No) for every application. This field allows us to map past inputs to loan outcomes, to be used in the future for a supervised machine learning. The Excel file, currently maintained by the Sales team, contains additional loan application details. However, its reliance on a shared folder introduces risks of data duplication and missing values. Data cleaning efforts is necessary to ensure its suitability for analysis.</p>
</section>
<section id="user-requirements-and-benefits" class="level3">
<h3 class="anchored" data-anchor-id="user-requirements-and-benefits">User Requirements and Benefits</h3>
<p>Consulting with a range of AFS stakeholders is part of the development process for this data analysis tool. The tool’s main users will be new recruits in the analytics team. However, potential benefits also extend to other departments like Sales and Customer Service, to whom the insights gained would improve customer targeting and support</p>
</section>
<section id="code-development-challenges-and-benefits" class="level3">
<h3 class="anchored" data-anchor-id="code-development-challenges-and-benefits">Code Development Challenges and Benefits</h3>
<p>The project acknowledges challenges associated with code development and maintenance. Our team, which consists of programmers with different degrees of expertise, will work to produce reusable, well-documented code. This strategy will guarantee effective maintenance by team members with a variety of skill sets in addition to making future improvements easier.</p>
</section>
<section id="regulatory-and-ethical-considerations" class="level3">
<h3 class="anchored" data-anchor-id="regulatory-and-ethical-considerations">Regulatory and Ethical Considerations</h3>
<p>Additionally, we understand how crucial ethical issues are when handling sensitive data. Throughout the development and implementation of this solution, we will respect data privacy and comply with all applicable rules.</p>
<p>The suggested approach would streamline the loan application process, enhance resource allocation within the lending team, and may shorten processing time. Furthermore, the data analysis provides significant insights that may be used to influence future marketing campaigns and improve customer support encounters. Failure to solve these difficulties, on the other hand, may result in lengthier approval delays, greater operational risks, and, ultimately, hampered AFS’s development potential.</p>
<p>This report details the proposed approach, key findings from the data analysis, and specific recommendations for improving the loan application process at AFS. It also addresses potential limitations of the study and outlines opportunities for further investigation.</p>
</section>
</section>
<section id="approach" class="level2">
<h2 class="anchored" data-anchor-id="approach">Approach</h2>
<p>This section outlines the approach for analyzing a loan dataset to gain insights into loan applications, applicant characteristics, and potential factors influencing loan approvals. The analysis will leverage Python libraries like pandas for data manipulation and Seaborn for visualization.</p>
<p>The approach follows the CRISP-DM (Cross-Industry Standard Process for Data Mining) framework:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/CRISP-DM-data-mining-framework.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: CRISP-DM Framework (Tounsi et al.&nbsp;,2020)</figcaption>
</figure>
</div>
<p>The first 3 parts would be explained in the following sections while the latter part of the framework comprise the recommendation section</p>
<section id="business-understanding" class="level3">
<h3 class="anchored" data-anchor-id="business-understanding">Business Understanding</h3>
<ul>
<li><strong>Increased Loan Applications:</strong> AFS has experienced a significant rise (over 200%) in loan applications due to a successful online marketing campaign focused on SEO and digital advertising.</li>
<li><strong>Manual Loan Process:</strong> Despite the digital marketing efforts, the loan application process remains manual, with applicants submitting personal information online and loan officers manually reviewing and approving/rejecting applications.</li>
<li><strong>Resource Constraints:</strong> The recent surge in applications has overwhelmed the loan team, leading to skills shortages, longer processing times, and increased operational and control risk.</li>
<li><strong>Growth Constraints:</strong> The current manual process hinders further business growth.</li>
</ul>
<p><strong>Business Needs:</strong></p>
<ul>
<li><strong>Automate Loan Processing:</strong> Implement a scalable solution to automate the loan processing workflow, reducing reliance on manual review by loan officers.</li>
<li><strong>Improve Approval Efficiency:</strong> Reduce approval times by automating decision-making based on relevant criteria.</li>
<li><strong>Mitigate Risk:</strong> Minimize operational and control risks associated with a manual process.</li>
<li><strong>Support Growth:</strong> Develop a solution that can handle future increases in loan applications without compromising efficiency or control.</li>
</ul>
</section>
<section id="data-understanding" class="level3">
<h3 class="anchored" data-anchor-id="data-understanding">Data Understanding</h3>
<p>The data will be obtained from two sources:</p>
<ol type="1">
<li><p><strong>Apex Loans Data.xlsx - an Excel File:</strong> This file contain loan information for a set of applicants in a structured format, including columns for applicant details, loan characteristics, and approval status.</p></li>
<li><p><strong>Loans_Database_Table.pdf - a PDF file:</strong> The PDF contains several pages of tables with loan applicant data, much like the Excel file.</p></li>
</ol>
</section>
<section id="data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation">Data preparation</h3>
<section id="data-loading-and-preprocessing" class="level4">
<h4 class="anchored" data-anchor-id="data-loading-and-preprocessing">Data Loading and Preprocessing</h4>
<ol type="1">
<li><p><strong>Loading Excel Data:</strong> We will use the <code>pandas.read_excel</code> function to import the Excel file into a pandas DataFrame. This DataFrame provides a tabular representation of the loan data for further processing.</p></li>
<li><p><strong>Loading PDF Data:</strong> The <code>tabula-py</code> library was used to extract data from the PDF tables. The <code>tabula.read_pdf</code> loaded in the data, recieving as a parameter, the pages to be read. This function returns a list of DataFrames, each representing a separate table from the PDF.</p></li>
<li><p><strong>PDF Data Cleaning:</strong> The next step involves addressing inconsistencies between the Excel and PDF data structures. The code identifies and rectifies a common issue where the first data row in each PDF table might be mistakenly used as column headers for subsequent rows. A function, <code>concatenate_pdfs</code>, was designed to handle this:</p>
<ul>
<li>It iterates through the list of DataFrames from the PDF.</li>
<li>For each DataFrame, it extracts the first row (misplaced headers) and create a new DataFrame with this row as the actual header.</li>
<li>It then combines the remaining rows (data) of the original DataFrame with the newly created header DataFrame.</li>
<li>The function ensures consistent column names are applied for all PDF data before concatenation.</li>
</ul></li>
<li><p><strong>Data Concatenation:</strong> Once both the Excel and processed PDF DataFrames are prepared, the <code>pandas.concat</code> function was used to merge them into a single comprehensive DataFrame. This DataFrame represents the combined loan dataset for further analysis.</p></li>
</ol>
</section>
<section id="data-cleaning-and-exploration" class="level4">
<h4 class="anchored" data-anchor-id="data-cleaning-and-exploration">Data Cleaning and Exploration</h4>
<ol type="1">
<li><p><strong>Data Quality Checks:</strong> Initial checks were performed using functions like <code>head()</code>, <code>shape</code>, and <code>info()</code> to assess the data structure, dimensions, data types, and identify potential missing values or duplicates. Setting the <code>Loan_ID</code> as the index simplifies data manipulation.</p></li>
<li><p><strong>Data Cleaning:</strong></p>
<ul>
<li><strong>Duplicate Removal:</strong> Duplicate rows identified using <code>drop_duplicates</code> were removed to ensure data integrity.</li>
<li><strong>Missing Values:</strong> The presence of missing values were checked using <code>isna().sum()</code>.</li>
<li><strong>Categorical Data Encoding:</strong> Categorical variables like marital status, employment status, or property area were mapped to meaningful labels for interpretability.</li>
<li><strong>Data Type Conversion:</strong> Data types for numerical columns were verified and converted to appropriate numerical types for appropriate visualisations.</li>
</ul></li>
</ol>
</section>
<section id="exploratory-data-analysis" class="level4">
<h4 class="anchored" data-anchor-id="exploratory-data-analysis">Exploratory Data Analysis:</h4>
<p>This section explores key characteristics of the loan dataset using various EDA techniques to gain insights into loan applications, applicant profiles, and potential factors influencing loan approvals.</p>
<p><strong>Distribution of Applicant Income and Loan Amount:</strong></p>
<ul>
<li><strong>Applicant Income:</strong> The distribution of applicant income exhibits a significant right skew, with most values concentrated between $10 and $10,000.</li>
<li><strong>Loan Amount:</strong> Excluding outliers exceeding $400,000, the distribution of loan amounts shows a slightly normal shape.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/afs-applicant%20income.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Distribution of applicant’s income</figcaption>
</figure>
</div>
<p><strong>Bivariate Analysis:</strong></p>
<ul>
<li><p><strong>Income and Loan Correlation:</strong> A slightly positive correlation (0.55) exists between applicant income and co-applicant income with loan amount. This suggests that higher combined income is associated with larger loan requests.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/applicant_incomevsloanamount.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Association between applicant’s income and the amount they loaned</figcaption>
</figure>
</div></li>
</ul>
<p><strong>Loan Performance:</strong></p>
<ul>
<li><strong>Total Loan Amount:</strong> Apex Financial Services (AFS) has disbursed a total of $89,659 in loans.</li>
<li><strong>Average Loan Characteristics:</strong> The average loan amount granted is $148.94 with a term of approximately 333 months.</li>
</ul>
<p><strong>Applicant Analysis:</strong></p>
<ul>
<li><strong>Approval Rates by Gender:</strong> There is a disparity in loan approvals by gender. Among approved loans, 82.5% were for male applicants, while 17.5% were for females. Conversely, rejected loans showed a similar breakdown, with 80.5% for males and 19.5% for females. This is not surprising since most loan applicants are male.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/gender_approvals.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Loan approvals. broken down by gender</figcaption>
</figure>
</div>
<ul>
<li><strong>Self-Employed Applicants:</strong> Self-employment appears uncommon within the applicant pool. However, for those who identified as self-employed, only 13.59% received loan approval. This suggests potential differences in approval criteria for self-employed applicants compared to salaried individuals.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/self_employement.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: Loan approvals, broken down by self-employment status</figcaption>
</figure>
</div>
<ul>
<li><p><strong>Top Loan Applicants:</strong> The top 10 applicants by loan amount share some commonalities. Notably, they are predominantly married men with graduate degrees and significantly higher incomes compared to the rest of the applicant pool, exceeding the 75th percentile for income.</p></li>
<li><p><strong>Property Area Distribution:</strong> Applicants were relatively evenly distributed across the three property areas (rural, suburban, semi-urban), with semi-urban dwellers having the greatest representation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/propery-areas.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: Distribution of Applicants Dwellings</figcaption>
</figure>
</div></li>
</ul>
</section>
</section>
<section id="code-libraries-used-and-rationale" class="level3">
<h3 class="anchored" data-anchor-id="code-libraries-used-and-rationale">Code Libraries Used and Rationale</h3>
<ul>
<li><p><strong>Pandas:</strong> This powerful library is a workhorse for data manipulation and exploration in Python. It provides functionalities like loading data from various sources (including Excel with <code>pd.read_excel</code>), cleaning and transforming data (including removing duplicates with <code>.drop_duplicates</code>), and performing various data analysis tasks. Pandas offers a user-friendly interface and efficient data structures, making it ideal for handling the loan dataset.</p></li>
<li><p><strong>Tabula-py:</strong> Extracting data from PDF tables can be challenging. Tabula-py simplifies this process by offering the <code>tabula.read_pdf</code> function. This function reads PDF documents and extract the tabular data within them.</p></li>
<li><p><strong>Seaborn (with Matplotlib dependency):</strong> Seaborn builds upon the foundation of Matplotlib, a fundamental Python library for creating various visualizations. Seaborn offers a higher-level interface, simplifying the creation of aesthetically pleasing and informative visualizations.</p></li>
<li><p><strong>Matplotlib</strong>: This fundamental python library was used to annotate and adjust the plots made with Seaborn.</p></li>
</ul>
</section>
<section id="choice-of-language-python-and-platform-google-colab" class="level3">
<h3 class="anchored" data-anchor-id="choice-of-language-python-and-platform-google-colab">Choice of Language (Python) and Platform (Google Colab)</h3>
<ul>
<li><p><strong>Python:</strong> Python is a popular choice for data analysis due to its readability, extensive ecosystem of libraries like Pandas, Seaborn, and many others, and its focus on clean and concise code. These features make Python ideal for data exploration and analysis, allowing the code to be well-structured and easily understood by others.</p></li>
<li><p><strong>Google Colab:</strong> Google Colab provides a free cloud-based Jupyter notebook environment. This platform eliminates the need for local software installation and offers access to powerful computing resources, making it suitable for working large datasets. Additionally, Colab facilitates collaboration and easy sharing of Python notebooks.</p></li>
</ul>
</section>
<section id="code-design-and-maintainability" class="level3">
<h3 class="anchored" data-anchor-id="code-design-and-maintainability">Code Design and Maintainability</h3>
<ul>
<li><p><strong>Pseudocode:</strong> Using pseudocode comments within the script can enhance readability and understanding, even for those unfamiliar with Python specifically. Pseudocode outlines the logic and steps involved in the analysis without getting bogged down in language syntax. This allows for easier comprehension and potential translation into other programming languages if needed. <img src="images/flowchart.png" class="img-fluid" alt="Figure 6: Flowchart showing programming solution"></p></li>
<li><p><strong>Version Control (Git):</strong> Utilizing version control systems like Git allows for tracking changes made to the code over time. This facilitates reverting to previous versions if necessary, identifying who made specific changes, and enabling collaboration.</p></li>
<li><p><strong>Code Comments:</strong> Including clear and concise comments within the code explains the purpose of different sections, clarifies variable names, and documents the overall logic. This improves readability and maintainability, allowing other members of the team to understand the code’s functionality and facilitating future modifications.</p></li>
</ul>
</section>
</section>
<section id="recommendations" class="level2">
<h2 class="anchored" data-anchor-id="recommendations">Recommendations</h2>
<p><strong>1. Data Preprocessing (Building upon Existing Work):</strong> The data cleaning and exploration steps outlined in the previous sections provide a solid foundation for model development.</p>
<p><strong>2. Feature Engineering:</strong></p>
<ul>
<li><strong>Feature Selection:</strong> The existing dataset contains various features. It’s crucial to identify the most relevant features that hold predictive power for loan approvals. Techniques like correlation analysis, chi-square tests, and feature importance scores from initial models can guide this selection process.</li>
<li><strong>Feature Creation:</strong> Creating new features based on existing ones should be considered. For example, a debt-to-income (DTI) ratio could be calculated by dividing total debt by income. Feature creation can help capture more complex relationships within the data.</li>
<li><strong>Data Encoding:</strong> Categorical variables like marital status or property area might require encoding into numerical representations suitable for machine learning algorithms. Techniques like one-hot encoding can be employed here.</li>
</ul>
<p><strong>3. Model Selection and Training:</strong></p>
<ul>
<li><strong>Tree-Based Models:</strong> Given the established dominance of tree-based models and their effectiveness with tabular data, XGBoost is a strong candidate (Shwartz-Ziv and Armon, 2022). XGBoost (Extreme Gradient Boosting) is a powerful and versatile ensemble method that excels at handling complex relationships and feature interactions. Its ability to handle missing values and perform automatic feature selection further adds to its appeal.</li>
<li><strong>Alternative Models:</strong> Other models like Random Forest, Logistic Regression, or Support Vector Machines (SVM) could also be explored for comparison. Evaluating different models helps identify the one that performs best on the specific loan dataset.</li>
</ul>
<p><strong>4. Model Training and Evaluation:</strong></p>
<ul>
<li><strong>Train-Test Split:</strong> The cleaned and preprocessed data needs to be divided into training and testing sets. The training set (typically 70-80% of the data) will be used to train the model, while the testing set (remaining 20-30%) will be used to evaluate its performance on unseen data.</li>
<li><strong>Model Training:</strong> The chosen model, say XGBoost, will be trained on the training data. Hyperparameter tuning, which involves adjusting model parameters to optimize performance, might be necessary. Tools like GridSearchCV can be used for this purpose.</li>
<li><strong>Evaluation Metrics:</strong> Once trained, the model’s performance will be evaluated on the testing set. Common metrics for binary classification problems like loan approval prediction include:
<ul>
<li><strong>Accuracy:</strong> Measures the overall proportion of correct predictions (approved/rejected) made by the model.</li>
<li><strong>Precision:</strong> Indicates the proportion of predicted approvals that were truly approved loans.</li>
<li><strong>Recall:</strong> Represents the proportion of actual approved loans that the model correctly predicted as approved.</li>
<li><strong>F1 Score:</strong> Combines precision and recall into a single metric, providing a balanced view of the model’s performance.</li>
<li><strong>Area Under the ROC Curve (AUC):</strong> Measures the model’s ability to distinguish between approved and rejected loans. A higher AUC indicates better discrimination.</li>
</ul></li>
</ul>
<p><strong>5. Model Deployment and Monitoring:</strong></p>
<ul>
<li><strong>Deployment:</strong> Based on the chosen model and evaluation results, a strategy for deploying the model into production can be developed. This could involve integrating the model into AFS’s loan processing system in a cloud environment like Docker to provide real-time loan approval predictions.</li>
<li><strong>Monitoring:</strong> It’s essential to monitor the model’s performance over time as new loan applications are submitted. Performance metrics can be tracked to identify any degradation in accuracy. Periodic retraining with fresh data might be necessary to maintain optimal performance.</li>
</ul>
<p><strong>Tools and Libraries:</strong></p>
<ul>
<li><strong>Scikit-learn:</strong> This machine learning library offers a wide range of algorithms, including XGBoost, Random Forest, Logistic Regression, and SVM. It also provides functionalities for data splitting, feature scaling, and model evaluation metrics.</li>
<li><strong>XGBoost:</strong> This specialized library offers an optimized implementation of the XGBoost algorithm, allowing for efficient training and hyperparameter tuning.</li>
<li><strong>Pandas and NumPy:</strong> These libraries will continue to be crucial for data manipulation and numerical computations during feature engineering and model evaluation.</li>
<li><strong>Matplotlib/Seaborn:</strong> While not directly used for model development, these libraries might be helpful for visualizing model performance metrics and feature importance scores.</li>
</ul>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">Reference</h2>
<p>Tounsi, Y., Anoun, H. and Hassouni, L. (2020) ‘CSMAS: Improving Multi-Agent Credit Scoring System by Integrating Big Data and the new generation of Gradient Boosting Algorithms’, pp.&nbsp;1-7. doi: 10.1145/3386723.3387851.</p>
<p>Shwartz-Ziv, R. and Armon, A., 2022. Tabular data: Deep learning is not all you need. Information Fusion, 81, pp.84-90.</p>
</section>
<section id="appendices" class="level2">
<h2 class="anchored" data-anchor-id="appendices">Appendices</h2>
</section>
<section id="appendix-1---notebook-contents" class="level2">
<h2 class="anchored" data-anchor-id="appendix-1---notebook-contents">Appendix 1 - Notebook contents</h2>
</section>
<section id="appendix-3---design-pseudocode" class="level2">
<h2 class="anchored" data-anchor-id="appendix-3---design-pseudocode">Appendix 3 - Design Pseudocode</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/flowchart.png" class="img-fluid figure-img"></p>
<figcaption>Design pseudocode</figcaption>
</figure>
</div>
<pre><code># Data Ingestion
Load Excel file data into DataFrame 'loans_xlsx'
Load PDF file data into a list of DataFrames 'loans_df_list'
Define a function 'concatenate_pdfs' to handle inconsistent headers in PDF files
    Process the first DataFrame as-is
    For the remaining DataFrames:
        Extract misplaced headers and create a new DataFrame
        Assign correct headers from the first DataFrame
        Concatenate with the remaining rows of the original DataFrame
        Append the corrected DataFrame to the 'processed_dfs' list
    Concatenate all DataFrames in 'processed_dfs' into a single DataFrame 'loans_pdf'
Combine 'loans_xlsx' and 'loans_pdf' into a single DataFrame 'loans'

# Data Cleaning and Preprocessing
Check for and remove duplicate rows in 'loans'
Handle missing values in 'loans'
Convert categorical variables to their corresponding labels
Convert data types to appropriate formats

# Exploratory Data Analysis (EDA)
Define a function 'visualize_outliers' to plot boxplots for numeric columns
    Create a grid of subplots based on the number of numeric columns
    Plot each numeric column in a separate subplot
Define a function 'plot_categorical_distributions' to plot count plots for categorical columns
    For each categorical column:
        Create a subplot and plot the count plot
Define a function 'plot_continuous_histograms' to plot histograms for continuous columns
    For each continuous column:
        Create a subplot and plot the histogram
Perform bivariate analysis using a pairplot

# Required Analyses
Calculate the total amount loaned, average loan amount, and average loan term
Analyze the applicant counts by approval status and gender
Investigate the gender distribution of approved and rejected loan applicants
Identify the maximum and minimum loan amounts and visualize the loan amount distribution
Calculate the percentage of self-employed applicants with approved loans
Analyze the income distribution of main applicants
Identify the top 10 applicants by loan amount
Visualize the distribution of property areas for loan applicants</code></pre>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>